---
title: "ETC3250/5250 Tutorial 3 Instructions"
subtitle: "Categorical response, and resampling"
author: "prepared by Professor Di Cook"
date: "Week 3"
output:
  html_document:
    after_body: tutorial-footer.html
    css: tutorial.css
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)

options(digits = 3)

library(emo)
```

# `r emo::ji("target")` Objective

The objectives for this week are to 

- explore the process of fitting a categorical response. 
- understand the the training/test split procedure for estimating your model error.
- practice plotting of data and models to help understand model fit, in particular boundaries between groups induced by a classification model.

# `r emo::ji("wrench")` Preparation 

Make sure you have these packages installed:

```
install.packages(c("tidyverse", "kableExtra", "tidymodels", "discrim", "mgcv", "patchwork"))
```

# `r emo::ji("book")` Reading 

- Textbook sections 4.3.1, 4.4.3, 5.1.3
- Alison Hill's [Take a Sad Script & Make it Better: Tidymodels Edition](https://kushagragpt99.github.io/post/2020-02-27-better-tidymodels/)

# `r emo::ji("waving_hand")` Getting started

If you are in a zoom tutorial, say hello in the chat. If in person, do say hello to your tutor and to your neighbours. 

```{r load_libraries, include=FALSE}
library(tidyverse)
library(tidymodels)
library(discrim)
library(mgcv)
library(patchwork)
library(kableExtra)

# Set default theme for document
ggplot2::theme_set(theme_bw())
```

# üí¨ Class discussion exercises

Textbook question, chapter 4 Q8

> "Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20 % on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. K = 1) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?"

- If we fit 1-NN, our model will be close to the training set (*i.e.* close to the training data)
  - *Training error rate will be 0%* for any training observation as the nearest neighbour will be the response itself 
  - Variance is extremely high (if we reshuffle the data points), model will change dramatically
- Since 1-NN has a test error of 36%, logistic regression will be chosen because of its lower test error rate

average error = (training error + test error) / 2
          18  = (0 + test error) / 2
   test error = 36%

# `r emo::ji("gear")` Exercise 

## 1. Fitting a logistic regression model to produce a spam filter

### a. 

Read about the `spam` data in [here](http://ggobi.org/book/chap-data.pdf). Read the data into R. Subset the data to contains these variables: `day of week`, `time of day`, `size.kb`, `domain`, `cappct`.  Set the levels of day of the week to match our regular week day order. Filter the data to contain only these domains "com", "edu", "net", "org", "gov".

```{r}
# --- Read-in data
spam <- read_csv("http://ggobi.org/book/data/spam.csv") 

# --- Clean data
spam <- spam %>% 
  
  # Convert day of week to factor; adjust levels to chronological order
  mutate(`day of week` = factor(`day of week`,
                                levels = c("Mon", "Tue", "Wed", "Thu",
                                           "Fri", "Sat", "Sun"))) %>% 
  
  # Filter to 5 common domains
  filter(domain %in% c("com", "edu", "net", "org", "gov")) %>%
  
  # Select relevant variables
  select(spam, 
         `day of week`, 
         `time of day`,
         size.kb, # Size of email
         domain, 
         cappct) # % of capital letters in subject line

# Convert character to factor variable
spam <- spam %>%
  mutate(spam = factor(spam))
```

**Data description**
- Collected by a statistics class in Iowa State University
- Each person monitored emails for a week (*e.g.* if it was spam, time of day email received, size of email)
- Goal was to build a *spam filter* that can predict if an email is 'spam' or 'ham'.

### b. 

Make some summary plots of these variables. 

```{r, fig.height = 4}
# Bar plot of Response variable
p1 <- ggplot(spam, aes(x=spam)) + geom_bar()

# Percent stacked bar chart
p2 <- ggplot(spam, aes(x = domain, fill = spam)) +
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "Dark2")

# Density plot
p3 <- ggplot(spam, aes(x = size.kb, colour = spam)) +
  geom_density() +
  scale_x_log10() +
  scale_colour_brewer(palette = "Dark2")

# Percent stacked bar chart
p4 <- ggplot(spam, aes(x = `day of week`, fill = spam)) +
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "Dark2")

# Percent stacked bar chart
p5 <- ggplot(spam, aes(x = `time of day`, fill = spam)) +
  geom_bar(position = "fill") +
  scale_fill_brewer(palette = "Dark2")

# Density plot
p6 <- ggplot(spam, aes(x = cappct, colour = spam)) +
  geom_density() +
  scale_x_log10() +
  scale_colour_brewer(palette = "Dark2")

(p1 + p2 + p3)/(p4 + p5 + p6)
```

**Choice of plots**  

- For *categorical* predictors, *relative proportion* of ham & spam are examined using 100%/percent stacked bar chart. If proportion are different, it suggests that variable is important for predicting spam.
- For *numerical* predictors, we determine if *distribution* of the variable is different for ham & spam. 
  - Density plots, histograms, side-by-side box plots are all reasonable.
  
**Notes on plots**  

- *Count of spam vs. ham*: There is more ham than spam 
- *Percent stacked Bar charts*: `domain`, `day of week`, `time of day` all have some differences in proportions of spam at some levels
  - Suggesting that variable may be important in predicting spam
- On average, `size.kb` appears to be larger for spam than ham
- There is little differences between the \% of capital letters in subject line (`cappct`) between spam & ham


### c. 

#### Fit logistic regression

Fit a logistic regression model for spam to remaining variables.

```{r}
# Set up model specification
logistic_mod <-
  logistic_reg() %>% 
  set_engine("glm") %>% 
  parsnip::translate()

# stats::glm()
```

**Step 1: Specify the type of model**
- Specify model, usually based on a mathematical structure
  
**Step 2: Specify the engine**
- Specify package or system to fit them model, Most often reflects software package
  
**Step 3: Declare the mode (if required)**
- Sets the class of the problem; usually influences how output will be collected
  - *e.g.* If outcome is numeric: `set_mode("regression")`
  - *e.g.* If outcome is qualitative/categorical: `set_mode("classification")`
- Step is not required if mode is already set in step 1

```{r}
# Fit logistic regression to all predictors
spam_fit <- 
  logistic_mod %>% 
  fit(spam ~ ., 
      data = spam)
```

#### Model summary

```{r}
# Extract estimated coefficients in a tidy tibble
tidy(spam_fit) %>% 
  kable() %>% 
  kable_styling(full_width = FALSE)
```

**Interpreting with log-odds ratio**

Example: Two predictors 

$$\begin{align*}\log(odds) &= \log(\frac{p}{1-p}) \\ &= \beta_0 + \beta_1 x_1 + \beta_2x_2 \end{align*}$$

- Holding $x_2$ fixed, a unit increase in $x_1$ increases the log-odds of spam by $\beta_1$.
  - Similar to regression model

**Findings**

- `Day of the week`, `time of day`, `domain` appears to be important variables while `size.kb` & `cappct` are not
- .net is the same fraction as .com, and when email comes from these domains, the probability that is is spam is higher (only positive coefficient estimate in domain)
- There is a higher chance that emails on Saturday (large & positive coefficient) is spam, and less chance if it arrives on Thursday (large & negative coefficient)
- There is a higher probability of spam in the early morning hours, & this decreases slightly as the day progresses
  - However, referring back to the plot of `time of day`, it may be better to include it as a categorical variable in order to account for the non-linear relationship
  - The r/s looks like it is high proportion in the early hours, low proportion in the working hours, & then a gradual increase in proportion through the evening

```{r}
# Extract model fit summary
glance(spam_fit) %>% 
  kable() %>% 
  kable_styling(full_width = FALSE)
```

- In thinking of the null model (`null.deviance`) as TSS and the model deviance (`deviance`) as RSS, the model explains about 50% (1228/2535)*100 of the variation in spam.

### d. 

By computing the model "deviance" for various choices of predictors, decide on an optimal number of these variables for predicting whether an email is spam. You could plot the model deviance against the different number of predictors. Explain why `null.deviance` is the same across all the models.

#### Saturated and null model

- **Saturated model**: Each data point has its own parameters ($n$ parameters to estimate)
    - Gives a sense of the ‚Äúbest case scenario‚Äù

- **Null model**: One parameter (*i.e.* only intercept & no predictors) to estimate for all data points
    - Gives a sense of the "worse case scenario"

- **Proposed (fitted) model**: Proposed data points with $p$ parameters + intercept ($p+1$ parameters to estimate)
    - Ideally, we want likelihood of data given the proposed model to be larger than the null model & close to the saturated model
    
$$\text{Residual deviance} = 2 \times \left[LL(saturated model) - LL(proposed model)\right]$$

$$\text{Null deviance} = 2 \times \left[LL(saturated model) - LL(null model)\right]$$

$$R^2 = 1 - RSS/TSS$$
$$R^2 = 1 - \text{residual deviance} / \text{null deviance}$$


#### Find optimal number of variables

```{r}
# --- Fit 3 models with varying no. of predictors

# 3 predictors
spam_fit1 <- 
  logistic_mod %>% 
  fit(spam ~ `day of week` + `time of day` + 
        domain, 
      data = spam)

# 4 predictors - add `cappct`
spam_fit2 <- 
  logistic_mod %>%
  fit(spam ~ `day of week` + `time of day` + 
        domain + cappct, 
      data = spam)

# 5 predictors - add `cappct` & `size.kb` 
spam_fit3 <- 
  logistic_mod %>% 
  fit(spam ~ `day of week` + `time of day` + 
        domain + cappct + size.kb, 
      data = spam)
```

```{r}
spam_dev <- tibble(
  
  # Compare models with 3, 4 & 5 predictors
  vars = 3:5,
  
  # Obtain residual deviance
  deviance = c(
    glance(spam_fit1)$deviance,
    glance(spam_fit2)$deviance,
    glance(spam_fit3)$deviance
  ),
  
  # Residual deviance (RSS) / null deviance (TSS)
  prop = c(
    glance(spam_fit1)$deviance / glance(spam_fit1)$null.deviance,
    glance(spam_fit2)$deviance / glance(spam_fit1)$null.deviance,
    glance(spam_fit3)$deviance / glance(spam_fit1)$null.deviance)
  )
```

- Null deviance is the variance of the model where spam proportion is the same across all levels/values of the variables (*i.e.* how well model is predicting `spam` with just an intercept).
  - it is the same across all 3 models

```{r}
# Compare deviance across the models
p7 <- 
  ggplot(spam_dev, aes(x = factor(vars), y = deviance, group = 1)) +
  geom_line() +
  geom_point()

# Compare proportion of null deviance 
p8 <-
  ggplot(spam_dev, aes(x = factor(vars), y = prop)) +
  geom_bar(stat = "identity") +
  ylab("Proportion of null deviance")

p7 + p8
```

- Model with three predictors (`domain`, `day of week` & `time of day`) is as effective as the model with 5 variables.

#### Likelihood ratio test (additional information)

- Test if difference in residual deviance is significant

**Assumptions**

1. Smaller model is correct
2. Models are *nested*
3. Distributional assumptions are true

---

*Example*: Comparing large model (model 2) and smaller nested model (Model 1)

**Model 1**: $\eta = \beta_0 + \beta_1x_1$
**Model 2**: $\eta = \beta_0 + \beta_1x_1 + \beta_2x_2$

Assuming model 1 is correct,

$$H_0: \beta_2  = 0 \\ H_1: \beta_2 \ne 0$$

---

**Steps**


*Step 1*: Compute difference in (residual) Deviance (likelihood ratio test)

$$\begin{align*}D_1 - D_2 = 2\log{\left(\frac{L_2}{L_1}\right)}  \sim \chi^2_{q_2-q_1} \end{align*}$$

*Step 2:*: Check test statistic $\chi^2$ & its $p$-value

If p-value < 0.05, the difference in deviance is significant at the 5% significance level.

--- 

With our data:

```{r}
# --- Step 1: Compute residual variance

# Option 1: glance()

# 3 predictors
glance(spam_fit1)$deviance # 1231 (df = 12)

# 5 predictors
glance(spam_fit3)$deviance # 1228 (df = 14)

# Option 2: Manual calculation
-2*logLik(spam_fit1$fit) # 1231 (df = 12)
-2*logLik(spam_fit3$fit) # 1228 (df = 14)
```

```{r}
# Compute p-value for chi-square test statistic
# Note: Degrees of freedom = difference in no. of parameters between the two models

# Option 1: anova()
anova(spam_fit1$fit, spam_fit3$fit, test = "Chisq")

# Option 2: Manual calculation
1 - pchisq( (-2*logLik(spam_fit1$fit) - -2*logLik(spam_fit3$fit) ), df = 2)
```

- We fail to reject the null hypothesis at the 5% level (p-value > 0.05)
- Therefore, adding `cappct` & `size.kb` did not improve the model fit significantly and the model with 3 predictors is the better model

### e. 

Compute the confusion table and report the classification error for your model. What proportion of ham (good emails) would end up in the spam folder?

```{r}
# --- Compute confusion table

# Extract predicted class
spam_pred <- 
  broom::augment(spam_fit1, new_data = spam)

# Create confusion table
spam_pred %>%
  count(spam, .pred_class) %>%
  pivot_wider(names_from = spam, values_from = n) %>%
  kable() %>%
  kable_styling(full_width = FALSE) 
  
# One or more common metrics for classification regression
yardstick::metrics(data = spam_pred, 
                   truth = spam,
                   estimate = .pred_class) %>%
  kable() %>%
  kable_styling(full_width = FALSE)
```


### f.

Conduct $k$-fold cross-validation on the model fitting and estimate the test classification error.

```{r}
set.seed(2021)

# Split data in 10 folds
# Stratified cross validation ensures each fold is a good representation of the original data
spam_folds <- vfold_cv(data = spam, 
                       v = 10,
                       strata = spam) 

compute_fold_error <- function(split) {
  
  # --- For each split
  
  # Split into analaysis and assessment set
  train <- analysis(split)
  test <- assessment(split)
  
  # Fit logistic regression model
  fit <- 
    logistic_mod %>% 
    fit(spam ~ `day of week` + `time of day` + domain, 
        data = train)
  
  # Obtain predicted for test set
  test_pred <- augment(fit, new_data = test)
  
  # Compute classification error
  error <- tibble(error = 1 - accuracy(data = test_pred, 
                                       truth = spam, 
                                       estimate = .pred_class)$.estimate) %>%
    # Add sample id (e.g. Fold01)
    rsample::add_resample_id(split = split)

  return(error)
}
```

```{r}
kfold_results <- 
  # Compute classification error for each fold
  # Bind all rows to output a single data frame
  map_df(
    .x = spam_folds$splits, 
    .f = ~ compute_fold_error(.x))

# Average of k classification error
kfold_results %>% summarise(mean(error))
```

**`analysis` and `assessment` set**
- Model is fit with `analysis` set (training set) 
- Model is evaluated with `assessment` set (test set)
- Analogous to training and test sets
  - Term is used to avoid confusion with initial split of data 

**Steps to k-fold cross-validation:**

*Example:* computing cross-validation classification error $E$

$$CV_{(k)} = \frac{1}{k} \sum_{i=1}^{n} E_k$$
1. Divide data into $k$ groups/folds of approximately equal size
2. Reserve one subset as validation set & train model on remaining $k-1$ folds
3. Test model (*e.g.* compute classification error) on reserved subset & record prediction error
4. Repeat $k$ times ‚Äì *i.e.* until each of $k$ subsets have served as test set
5. Compute average of $k$ classification error ‚Äì serve as performance metric for the model
  
#### What is the function doing? 

```{r, eval=FALSE}
# What is the function doing?

# --- Fit 1 of 10

split1 <- spam_folds %>% pluck("splits", 1) # spam_folds$splits[[1]] 
train1 <- analysis(split1)
test1 <- assessment(split1)

# Model fit
fit1 <- 
  logistic_mod %>% 
  fit(spam ~ `day of week` + `time of day` + domain, 
        data = train1) # Fit on training set of first fold

# Get predictions for test set
test_pred1 <- augment(fit1, new_data = test1)

# Extract classification test error
tibble(error = 1 - accuracy(data = test_pred1, 
                            truth = spam, 
                            estimate = .pred_class)$.estimate) %>%
    rsample::add_resample_id(split = split1)

# --- Fit 2 of 10

split2 <- spam_folds$splits[[2]]
train2 <- analysis(split2)
test2 <- assessment(split2)

fit2 <- logistic_mod %>% 
  fit(spam ~ `day of week` + `time of day` + domain, 
        data = train1) # Fit on new training set

test_pred2 <- ...
```

- We do not want to repeat steps for all 10 folds, so we create `compute_fold_error` function to automate the process.

### g. 

Explain why linear discriminant analysis (LDA) would not be suitable for modelling spam.

- LDA assumes predictors have a normal distribution
  - It cannot be used with categorical predictors


## 2. Fitting various catgorical response models to food quality data

a. Read about the `olive` data in [here](http://ggobi.org/book/chap-data.pdf). Download the data, filter the data to remove Southern oils, and select just the variables, region, oleic and linoleic. Make a plot of oleic vs linoleic, coloured by region. (You will need to set region to be a factor variable.) Split the data into 2/3 training and 1/3 test sets. When you make the split for a classification data set, why should you ensure that each level of your response variable is sampled in the same training/test proportion?

```{r}
# Read-in data
olive <- read_csv("http://ggobi.org/book/data/olive.csv") %>%
  # Remove southern oil
  filter(region != 1) %>% 
  select(region, oleic, linoleic) %>%
  mutate(region = factor(region))

ggplot(olive, aes(x=oleic, y=linoleic, colour=region)) +
  geom_point() + 
  scale_colour_brewer(palette="Dark2") +
  theme(aspect.ratio=1)
```

```{r}
set.seed(775)

# Allocate 2/3 of the data as training, 1/3 as test set
olive_split <- initial_split(olive, 2/3, strata = region)
olive_train <- training(olive_split)
olive_test <- testing(olive_split)
```

- As we have imbalance classes, random sample may result in different proportions of the response in the training and test samples
- In extreme cases, one class is really small, the training sample may contain only one class
- Therefore, stratified sampling ensure that each class is represented in both training and test sets in the desired proportion

### b. 

Fit a logistic regression. Compute the training and test error. 

```{r warning = TRUE}
olive_log_fit <- 
  logistic_mod %>% 
  fit(region ~ ., 
      data = olive_train)
```

```{r}
tidy(olive_log_fit) %>%
  kable() %>%
  kable_styling(full_width = FALSE)

glance(olive_log_fit) %>%
  kable() %>%
  kable_styling(full_width = FALSE)
```

$\eta = -996.6010 + 0.2148.oleic - 0.5796.linoleic$

```{r}
# --- Additional code
olive_aug <- augment(olive_log_fit$fit, 
                     type.predict = "response", 
                     new_data = olive_train) %>% 
  mutate(y = if_else(condition = region == 2,
                     true = 0,
                     false = 1)) 

# Use .fitted values from augment()
olive_aug %>% 
  ggplot(aes(x = oleic, y = y)) +
  geom_point() +
  geom_line(aes(x = oleic, y = .fitted))

# Use stat_smooth function
olive_train %>% 
  mutate(region = if_else(region == 2,
                          true = 0,
                          false = 1)) %>% 
  ggplot(aes(x = oleic, y = region)) +
  geom_point() +
  stat_smooth(method = "glm", method.args = list(family = binomial), se = FALSE)
```


```{r}
# Training error
olive_log_tr_pred <- olive_train %>%
  mutate(.pred_class = predict(olive_log_fit, olive_train)$.pred_class)

olive_log_tr_pred %>% 
  count(region, .pred_class) %>%
  pivot_wider(names_from = region, 
              values_from = n, values_fill=0) %>%
  kable() %>%
  kable_styling(full_width = FALSE)

# Test error
olive_log_pred <- augment(olive_log_fit, new_data = olive_test)

olive_log_pred %>% 
  count(region, .pred_class) %>%
  pivot_wider(names_from = region, 
              values_from = n, values_fill=0) %>%
  kable() %>%
  kable_styling(full_width = FALSE)

metrics(data = olive_log_pred, 
        truth = region, 
        estimate = .pred_class) %>%
  kable() %>%
  kable_styling(full_width = FALSE)
```

### c. 

Fit a linear discriminant classifier. Compute the training and test error. 

```{r}
lda_mod <- discrim_linear() %>% 
  set_engine("MASS") %>% 
  translate()

olive_lda_fit <- 
  lda_mod %>% 
  fit(region ~ ., 
      data = olive_train)
olive_lda_fit

# Training error
olive_lda_tr_pred <- olive_train %>%
  mutate(.pred_class = 
           predict(olive_lda_fit, olive_train)$.pred_class)
olive_lda_tr_pred %>% 
  count(region, .pred_class) %>%
  pivot_wider(names_from = region, 
              values_from = n, values_fill=0) %>%
  kable() %>%
  kable_styling(full_width = FALSE)

# Testing error
olive_lda_pred <- olive_test %>%
  mutate(.pred_class = 
           predict(olive_lda_fit, olive_test)$.pred_class)

olive_lda_pred %>% 
  count(region, .pred_class) %>%
  pivot_wider(names_from = region, 
              values_from = n, values_fill=0) %>%
  kable() %>%
  kable_styling(full_width = FALSE)
metrics(olive_lda_pred, truth = region, 
        estimate = .pred_class) %>%
  kable() %>%
  kable_styling(full_width = FALSE)
```

### d. 

Examine the boundaries between groups. Generate a grid of points between the minimum and maximum values for the two predictors. Predict the region at these locations. Make a plot of the this data, coloured by predicted region. Compare the two boundaries. 

```{r fig.height = 3}
# Create hypothetical data
grid <- expand_grid(oleic = seq(6800, 8500, 25),
                    linoleic = seq(500, 1500, 25))

olive_grid <- augment(olive_log_fit, new_data = grid)

olive_grid <- olive_grid %>%
  mutate(.pred_class_lda = predict(olive_lda_fit, olive_grid)$.pred_class)

p9 <- ggplot(olive_grid) +
  geom_point(aes(x = oleic, y = linoleic,
                 colour = .pred_class)) +
  geom_point(data = olive_test,
             aes(x = oleic, y = linoleic, shape = region)) +
  scale_colour_brewer(palette = "Dark2") +
  theme_bw() + 
  ggtitle("Logistic")

p10 <- ggplot(olive_grid) +
  geom_point(aes(x = oleic, y = linoleic,
                 colour = .pred_class_lda)) +
  geom_point(data = olive_test,
             aes(x = oleic, y = linoleic, shape = region)) +
  scale_colour_brewer(palette = "Dark2") +
  theme_bw() +
  ggtitle("LDA")

p9 + p10
```

## `r emo::ji("stop_sign")` Wrapping up

Talk to your tutor about what you think you learned today, what was easy, what was fun, what you found hard.

##### ¬© Copyright 2022 Monash University
