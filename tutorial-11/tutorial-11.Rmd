---
title: "ETC3250/5250 Tutorial 11 Instructions"
subtitle: "Cluster summaries"
author: "prepared by Professor Di Cook"
date: "Week 11"
output:
  html_document:
    after_body: tutorial-footer.html
    css: tutorial.css
---

```{r, echo = FALSE, message = FALSE, warning = FALSE, warning = FALSE}
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  error = FALSE, 
  collapse = TRUE,
  comment = "#",
  fig.height = 4,
  fig.width = 8,
  fig.align = "center",
  cache = FALSE
)
```

# ðŸ–² Objective

The objectives of this tutorial are to

* summarise cluster analysis results
* compare and contrast results from different clustering

Refer to the results from tutorial 10. This tutorial builds on that work.

# 1. Conduct cluster analysis on simple data


Use the flea data that comes in the `tourr` package, and where we know the true classes. This is the data also used in class examples.

```{r, include=FALSE}
library(tidyverse)
library(tourr)
library(ggdendro)
library(fpc)
library(lubridate)

# Set default theme for the document
ggplot2::theme_set(theme_bw())

select <- dplyr::select
```


## a.

Produce the clustering into three groups from (i) Wards, (ii) $k$-means, $k=3$, and make a confusion table to compare the two results.

```{r}
# Read-in data
data(flea, package = "tourr")

# Standardise variables
flea_std <- flea %>%
  mutate(across(where(is.numeric), ~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE) ))
```


```{r}
# --- Hierarchical clustering with ward.D2 method

# Compute distance matrix
flea_dist <- 
  flea_std %>% 
  select(where(is.numeric)) %>% 
  dist()

# Hierarchical clustering with ward method
flea_hc_w <- hclust(flea_dist, method = "ward.D2")

# Draw dendrogram
# ggdendro::ggdendrogram(flea_hc_w, rotate = TRUE)

# Add predictions onto dataset
flea_std <- 
  flea_std %>%
  mutate(cl_w = cutree(flea_hc_w, 3)) # k = 3
```

```{r}
# --- K means

set.seed(2021) 

flea_km <- 
  stats::kmeans(flea_std[,1:6], 
                centers = 3) # k = 3

# Add predictions onto dataset
flea_std <- 
  flea_std %>% 
  mutate(cl_km = flea_km$cluster)
```

## b.

Map the cluster labels from the two results, and calculate the agreement. 

```{r}
flea_std %>% 
  count(cl_w, cl_km) %>%
  pivot_wider(names_from = cl_w, 
              values_from = n, 
              values_fill = 0)
```

```{r}
flea_km$centers
# Compute mean of each cluster
flea_km$centers

# Compute means for HC
flea_std %>% 
  group_by(cl_w) %>% 
  summarise(across(.cols = colnames(flea_std)[1:6],
                   ~ mean(.x)))
```


# 2. Cluster statistics graduate programs 

Remember the National Research Council ranking of Statistics graduate programs data. This data contained measurements recorded on departments including total faculty, average number of PhD students, average number of publications, median time to graduate, and whether a workspace is provided to students. These variables can be used to group departments based on similarity on these characteristics.

## a. 

Read the data, handle missing values, select the variables that can be used, and standardise these variables. Use Euclidean distance and Wards linkage to conduct a cluster analysis, on the full set of variables, and on a reduced set of `Average.Publications`, `Average.Citations`, `Faculty.with.Grants.Pct`, `Awards.per.Faculty`, `Median.Time.to.Degree`, `Ave.GRE.Scores.` Make a confusion matrix to compare the six cluster results from the full set, and the five cluster result from the reduced set (Hubert and wb.ratio suggest 5 clusters). 

```{r, include=FALSE}
# Read the data
nrc <- read_csv(here::here("data/nrc.csv"))

# Data preprocessing
nrc_vars <- nrc %>%
  select(Institution.Name,
         Average.Publications:Student.Activities) %>%
  # Academic.Plans.Pct dropped as it has 19 missing departments
  select(-Academic.Plans.Pct) %>%
  # NA probably imply specific occasion/department is not included in the university
  replace_na(list(Tenured.Faculty.Pct = 0, 
                  Instruction.in.Writing = 0,
                  Instruction.in.Statistics = 0,
                  Training.Academic.Integrity = 0,
                  Acad.Grievance.Proc = 0,
                  Dispute.Resolution.Proc = 0))

# Standardise variables
nrc_vars_std <- nrc_vars %>% 
  mutate(across(where(is.numeric), ~ (.x - mean(.x, na.rm = TRUE)) / sd(.x, na.rm = TRUE) ))
```

## b.

Map the cluster labels from the two results, and calculate the agreement. 

### Full set of variables

```{r}
# (1) Full set of variables 

# Compute distance matrix
nrc_vars_dist1 <-
  nrc_vars_std %>% 
  select(-Institution.Name) %>% 
  dist(method = "euclidean")

# Hierarchical clustering using ward method
nrc_hc_w1 <- hclust(nrc_vars_dist1, method = "ward.D2")
nrc_hc_w1$labels <- nrc_vars$Institution.Name
```

```{r}
compute_stats_nrc <- function(k, hc, dist) {
  
  # Create cluster membership with k cluster solution
  cl <- cutree(hc, k = k)
  
  # Obtain cluster validation statistics
  x <- fpc::cluster.stats(dist, clustering = cl)
  
  # Extract relevant statistic for the k cluster solution
  tibble(
    k = k,
    within.cluster.ss = x$within.cluster.ss,
    wb.ratio = x$wb.ratio,
    ch = x$ch,
    pearsongamma = x$pearsongamma,
    dunn = x$dunn,
    dunn2 = x$dunn2
  )
}
```

```{r}
# Obtain cluster statistics for 2 to 10 cluster solution
nrc_hc_cl_stats <-
  purrr::map_dfr(2:10, ~ compute_stats_nrc(k =.x, 
                                           hc = nrc_hc_w1,
                                           dist = nrc_vars_dist1))

# Convert to long format for plotting
nrc_hc_cl_stats_long <- nrc_hc_cl_stats %>% 
  pivot_longer(-k, 
               names_to ="stat",
               values_to = "value")

# Plot cluster statistics
nrc_hc_cl_stats_long %>% 
  ggplot() + 
  geom_line(aes(x = k, y = value)) +
  labs(x = "# clusters", y = "") +
  facet_wrap(~ stat, ncol = 3, scales = "free_y") 
```

* `wb.ratio`: Average within cluster / Average between cluster
  * preferably **low**, always drops for each additional cluster so look for large drops
  
* `pearson gamma`: Correlation between distances, a 0 and 1 vector where 0 means same cluster, 1 means different clusters
  * If distance correlation $DCov(X,Y)$ is 0, X and Y are independent.
  * preferably **high**

### Reduced set of variables

```{r}
# (2) Reduced set of variables 

# Compute distance matrix
nrc_vars_dist2 <-
  nrc_vars_std %>% 
  select(Average.Publications,
         Average.Citations,
         Faculty.with.Grants.Pct,
         Awards.per.Faculty,
         Median.Time.to.Degree,
         Ave.GRE.Scores) %>% 
  dist(method = "euclidean")

# Hierarchical clustering using ward method
nrc_hc_w2 <- hclust(nrc_vars_dist2, method = "ward.D2")
nrc_hc_w2$labels <- nrc_vars$Institution.Name
```

```{r}
# Obtain cluster statistics for 2 to 10 cluster solution
nrc_hc_cl_stats <- purrr::map_dfr(2:10, ~ compute_stats_nrc(k =.x, 
                                                            hc = nrc_hc_w2,
                                                            dist = nrc_vars_dist2))

# Convert to long format for plotting
nrc_hc_cl_stats_long <- nrc_hc_cl_stats %>% 
  pivot_longer(-k, 
               names_to ="stat",
               values_to = "value")

# Plot cluster statistics
nrc_hc_cl_stats_long %>% 
  ggplot() + 
  geom_line(aes(x = k, y = value)) +
  labs(x = "# clusters", y = "") +
  facet_wrap(~ stat, ncol = 3, scales = "free_y") 
```

* Based on pearson gamma & wb.ratio statistics, 5-cluster solution is the best.

### Calculate agreement

```{r}
# Add predictions to actual (non-standardised) dataset
nrc_vars <- 
  nrc_vars %>%
  mutate(cl_w1 = cutree(nrc_hc_w1, 6),
         cl_w2 = cutree(nrc_hc_w2, 5))

# Compute agreement
nrc_vars %>%
  count(cl_w1, cl_w2) %>%
  pivot_wider(names_from = cl_w1, 
              values_from = n, 
              values_fill = 0)
```

* w2 cluster 2 most matches w1 cluster 3, w2 cluster 3 most matches w1 cluster 1 BUT beyond this it's not possible to map the two solutions. 
* The two solutions REALLY don't agree.

## c.

Draw a scatterplot matrix (or a parallel coordinate plot) of the results from the smaller subset of variables. Describe how the clusters differ from each other.

```{r}
nrc_subset <- nrc_vars %>% 
  select(Average.Publications,
         Average.Citations,
         Faculty.with.Grants.Pct,
         Awards.per.Faculty,
         Median.Time.to.Degree,
         Ave.GRE.Scores,
         cl_w2) %>% 
  mutate(cl_w2 = as.factor(cl_w2))
```

```{r eval = FALSE, echo = FALSE}
# Scatter plot matrix
nrc_subset %>% GGally::ggscatmat(color = "cl_w2")

# Parallel coordinate plot maybe useful too
GGally::ggparcoord(nrc_subset,
                   groupColumn = "cl_w2", 
                   order = "anyClass") +
  facet_wrap(~ nrc_vars$cl_w2, ncol = 1) +
  theme(legend.position = "none",
        axis.text.y = element_blank())

GGally::ggparcoord(nrc_vars, columns = c(2,3,4,5,8,17), groupColumn = 51, 
           order = "anyClass") + 
  facet_wrap(~nrc_vars$cl_w2, ncol=1) +
  theme(legend.position = "none")

nrc_subset %>% 
  select(Average.Citations, Average.Publications, cl_w2) %>% 
  group_by(cl_w2) %>% 
  summarise(corr = cor(Average.Citations, Average.Publications))

nrc_subset %>% 
  select(Average.Citations, Average.Publications, cl_w2) %>% 
  group_by(cl_w2) %>% 
  ggplot() +
  geom_point(aes(x = Average.Citations, y = Average.Publications)) +
  facet_wrap(~ cl_w2)
  # summarise(corr = cor(Average.Citations, Average.Publications))
```

* This is a very difficult clustering problem because there are no naturally separated clusters in the data.

* Cluster 1 tends to have low average GRE scores (entrance exams for graduate students), but is otherwise mixed values on other variables.

* Cluster 4 has relatively high median time to degree, but mixed on other variables.

* Cluster 5, which has only one observation (Stanford University) which is an outlier relative to the other institutions. It has a large number of awards per faculty, and also a high publication and citation rate.**

## d.

Compute the means of the clusters. Describe how the clusters differ from each other, based on these values.

```{r}
nrc_means <-
  nrc_subset %>% 
  group_by(cl_w2) %>% 
  summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)))

nrc_means %>% 
  slice(-5)

nrc_subset %>% 
  mutate(Institution.Name = nrc_vars$Institution.Name,
         .before = "Average.Publications") %>% 
  filter(cl_w2 == 4)
```

**Similar summary to what is learned from the plots.**

* Cluster 2 has high scores for faculty, but less impressive (similar to other clusters) records for students.

* Cluster 4 has low scores for faculty, and although tends to recruit high-performing students, it takes them longer to graduate. These might not be the best institutions to attend as a graduate students.

* Cluster 5 (just the one institution) is highly rated for faculty, and students.


##### Â© Copyright 2022 Monash University
